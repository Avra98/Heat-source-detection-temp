{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "900\n",
      "Epoch:1| T Loss:-0.0000590547 | V Loss:-38532.9492187500\n",
      "Epoch:2| T Loss:-84004666955.0933380127 | V Loss:-836213604352.0000000000\n",
      "Epoch:3| T Loss:-1277510432995.5556640625 | V Loss:-9196233818112.0000000000\n",
      "Epoch:4| T Loss:-5352196760316.5869140625 | V Loss:-33597503307776.0000000000\n",
      "Epoch:5| T Loss:-11863755517678.9335937500 | V Loss:-80569870319616.0000000000\n",
      "Epoch:6| T Loss:-24107004801260.6562500000 | V Loss:-144765882466304.0000000000\n",
      "Epoch:7| T Loss:-37789229190275.9843750000 | V Loss:-243419754528768.0000000000\n",
      "Epoch:8| T Loss:-56840255756916.0546875000 | V Loss:-368144396845056.0000000000\n",
      "Epoch:9| T Loss:-81043140930874.0312500000 | V Loss:-522637289717760.0000000000\n",
      "Epoch:10| T Loss:-106128403274865.7812500000 | V Loss:-704956000632832.0000000000\n",
      "Epoch:11| T Loss:-163581865829189.4062500000 | V Loss:-911335286112256.0000000000\n",
      "Epoch:12| T Loss:-218870273547086.5000000000 | V Loss:-1193128761491456.0000000000\n",
      "Epoch:13| T Loss:-278724250589375.1562500000 | V Loss:-1505787348254720.0000000000\n",
      "Epoch:14| T Loss:-422584199257638.6875000000 | V Loss:-1836271525691392.0000000000\n",
      "Epoch:15| T Loss:-420778518651426.1250000000 | V Loss:-2292697566019584.0000000000\n",
      "Epoch:16| T Loss:-439051867420244.1875000000 | V Loss:-2756623290335232.0000000000\n",
      "Epoch:17| T Loss:-554716825623520.1250000000 | V Loss:-3246558262853632.0000000000\n",
      "Epoch:18| T Loss:-694515300506737.7500000000 | V Loss:-3749213317890048.0000000000\n",
      "Epoch:19| T Loss:-863389215061879.5000000000 | V Loss:-4353938807586816.0000000000\n",
      "Epoch:20| T Loss:-916266409051058.6250000000 | V Loss:-5010881435926528.0000000000\n",
      "Epoch:21| T Loss:-973530883499986.5000000000 | V Loss:-5705076463108096.0000000000\n",
      "Epoch:22| T Loss:-973323274325269.6250000000 | V Loss:-6508227831267328.0000000000\n",
      "Epoch:23| T Loss:-1129852956564375.2500000000 | V Loss:-7351153280942080.0000000000\n",
      "Epoch:24| T Loss:-1373707073961037.2500000000 | V Loss:-8277324860489728.0000000000\n",
      "Epoch:25| T Loss:-1471851467085651.0000000000 | V Loss:-9304677279596544.0000000000\n",
      "Epoch:26| T Loss:-1568350105097411.7500000000 | V Loss:-10343556854054912.0000000000\n",
      "Epoch:27| T Loss:-2253477179757399.5000000000 | V Loss:-11359262932467712.0000000000\n",
      "Epoch:28| T Loss:-2470260404618526.5000000000 | V Loss:-12623469376176128.0000000000\n",
      "Epoch:29| T Loss:-2519658560565484.5000000000 | V Loss:-13862275383296000.0000000000\n",
      "Epoch:30| T Loss:-2576180482890793.0000000000 | V Loss:-15168769001259008.0000000000\n",
      "Epoch:31| T Loss:-2496396254140607.0000000000 | V Loss:-16498942921408512.0000000000\n",
      "Epoch:32| T Loss:-3179156324601910.5000000000 | V Loss:-18007048746696704.0000000000\n",
      "Epoch:33| T Loss:-3455090984388162.0000000000 | V Loss:-19516419439853568.0000000000\n",
      "Epoch:34| T Loss:-3372977928926367.5000000000 | V Loss:-21195197601808384.0000000000\n",
      "Epoch:35| T Loss:-4659626741169739.0000000000 | V Loss:-22824216420155392.0000000000\n",
      "Epoch:36| T Loss:-4070910954431083.0000000000 | V Loss:-24697485733658624.0000000000\n",
      "Epoch:37| T Loss:-5215450332143580.0000000000 | V Loss:-26515234004926464.0000000000\n",
      "Epoch:38| T Loss:-4815119370149924.0000000000 | V Loss:-28501875422658560.0000000000\n",
      "Epoch:39| T Loss:-5713734952536419.0000000000 | V Loss:-30554356541554688.0000000000\n",
      "Epoch:40| T Loss:-5139537606002752.0000000000 | V Loss:-32599540511014912.0000000000\n",
      "Epoch:41| T Loss:-6261896474564144.0000000000 | V Loss:-34968880694689792.0000000000\n",
      "Epoch:42| T Loss:-7212925776246720.0000000000 | V Loss:-37444714592468992.0000000000\n",
      "Epoch:43| T Loss:-7403273229247301.0000000000 | V Loss:-39646061425328128.0000000000\n",
      "Epoch:44| T Loss:-7082260425241614.0000000000 | V Loss:-42027895668867072.0000000000\n",
      "Epoch:45| T Loss:-8100229827342573.0000000000 | V Loss:-44700984529649664.0000000000\n",
      "Epoch:46| T Loss:-7647353204657502.0000000000 | V Loss:-47171269329682432.0000000000\n",
      "Epoch:47| T Loss:-8944716452293655.0000000000 | V Loss:-49749173125251072.0000000000\n",
      "Epoch:48| T Loss:-7821768734171609.0000000000 | V Loss:-52436723140919296.0000000000\n",
      "Epoch:49| T Loss:-10597157817523168.0000000000 | V Loss:-55516644188880896.0000000000\n",
      "Epoch:50| T Loss:-8696267750682424.0000000000 | V Loss:-58347701946810368.0000000000\n",
      "Epoch:51| T Loss:-12294908835253744.0000000000 | V Loss:-61711378599116800.0000000000\n",
      "Epoch:52| T Loss:-9107857100904712.0000000000 | V Loss:-64711125492563968.0000000000\n",
      "Epoch:53| T Loss:-11345470771232476.0000000000 | V Loss:-68053529171853312.0000000000\n",
      "Epoch:54| T Loss:-14230352276676936.0000000000 | V Loss:-71433578239492096.0000000000\n",
      "Epoch:55| T Loss:-11137821891929048.0000000000 | V Loss:-75086396385132544.0000000000\n",
      "Epoch:56| T Loss:-13572900520527862.0000000000 | V Loss:-78491248888905728.0000000000\n",
      "Epoch:57| T Loss:-14299148719226880.0000000000 | V Loss:-82414314966745088.0000000000\n",
      "Epoch:58| T Loss:-13516972029061658.0000000000 | V Loss:-86352447789858816.0000000000\n",
      "Epoch:59| T Loss:-15924017997870530.0000000000 | V Loss:-90678502059147264.0000000000\n",
      "Epoch:60| T Loss:-16212077827551978.0000000000 | V Loss:-94788699862073344.0000000000\n",
      "Epoch:61| T Loss:-17722536812927604.0000000000 | V Loss:-99039617973485568.0000000000\n",
      "Epoch:62| T Loss:-18487046794593640.0000000000 | V Loss:-102652604592422912.0000000000\n",
      "Epoch:63| T Loss:-18301169467199760.0000000000 | V Loss:-106885208963284992.0000000000\n",
      "Epoch:64| T Loss:-17827093038651584.0000000000 | V Loss:-111191892930068480.0000000000\n",
      "Epoch:65| T Loss:-21107185446943020.0000000000 | V Loss:-115844381893722112.0000000000\n",
      "Epoch:66| T Loss:-17896742213552848.0000000000 | V Loss:-120708432356442112.0000000000\n",
      "Epoch:67| T Loss:-22068960031427936.0000000000 | V Loss:-125525779344785408.0000000000\n",
      "Epoch:68| T Loss:-26678379101214764.0000000000 | V Loss:-130954935834509312.0000000000\n",
      "Epoch:69| T Loss:-22210428084175940.0000000000 | V Loss:-135721112582488064.0000000000\n",
      "Epoch:70| T Loss:-22253636447788364.0000000000 | V Loss:-140689066894032896.0000000000\n",
      "Epoch:71| T Loss:-23600855828506432.0000000000 | V Loss:-146485528986910720.0000000000\n",
      "Epoch:72| T Loss:-28237555003457972.0000000000 | V Loss:-151473137068802048.0000000000\n",
      "Epoch:73| T Loss:-26007503529770136.0000000000 | V Loss:-157010724302880768.0000000000\n",
      "Epoch:74| T Loss:-27700206567449632.0000000000 | V Loss:-161918411633328128.0000000000\n",
      "Epoch:75| T Loss:-29222233636829048.0000000000 | V Loss:-167848112201662464.0000000000\n",
      "Epoch:76| T Loss:-33446496226765120.0000000000 | V Loss:-174062551921852416.0000000000\n",
      "Epoch:77| T Loss:-28037976539502420.0000000000 | V Loss:-179547053360152576.0000000000\n",
      "Epoch:78| T Loss:-31398609396208036.0000000000 | V Loss:-185556674679930880.0000000000\n",
      "Epoch:79| T Loss:-38555203526725032.0000000000 | V Loss:-192153314949857280.0000000000\n",
      "Epoch:80| T Loss:-36154312379672528.0000000000 | V Loss:-198848258431254528.0000000000\n",
      "Epoch:81| T Loss:-40179460743895888.0000000000 | V Loss:-205286053630705664.0000000000\n",
      "Epoch:82| T Loss:-39669956829021176.0000000000 | V Loss:-211056067314974720.0000000000\n",
      "Epoch:83| T Loss:-39838653676391264.0000000000 | V Loss:-218004001549975552.0000000000\n",
      "Epoch:84| T Loss:-36666799636097272.0000000000 | V Loss:-224599696927096832.0000000000\n",
      "Epoch:85| T Loss:-36205957662508752.0000000000 | V Loss:-231027287284252672.0000000000\n",
      "Epoch:86| T Loss:-36897596900204144.0000000000 | V Loss:-238168769925480448.0000000000\n",
      "Epoch:87| T Loss:-35930390728278016.0000000000 | V Loss:-244696312761548800.0000000000\n",
      "Epoch:88| T Loss:-42644323920811568.0000000000 | V Loss:-252055601784291328.0000000000\n",
      "Epoch:89| T Loss:-41704334326711656.0000000000 | V Loss:-260163228728819712.0000000000\n",
      "Epoch:90| T Loss:-37369358245936056.0000000000 | V Loss:-267500510339137536.0000000000\n",
      "Epoch:91| T Loss:-44807170800450568.0000000000 | V Loss:-274573823259443200.0000000000\n",
      "Epoch:92| T Loss:-44836109946493632.0000000000 | V Loss:-282152756909703168.0000000000\n",
      "Epoch:93| T Loss:-52917515523928880.0000000000 | V Loss:-289962553642057728.0000000000\n",
      "Epoch:94| T Loss:-52972740327953312.0000000000 | V Loss:-297757300809007104.0000000000\n",
      "Epoch:95| T Loss:-55426439796808312.0000000000 | V Loss:-305907327670681600.0000000000\n",
      "Epoch:96| T Loss:-53916082210838352.0000000000 | V Loss:-314384012565020672.0000000000\n",
      "Epoch:97| T Loss:-54584384925073992.0000000000 | V Loss:-323006898146115584.0000000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:98| T Loss:-66219109106943848.0000000000 | V Loss:-331962763951734784.0000000000\n",
      "Epoch:99| T Loss:-55944656730493344.0000000000 | V Loss:-340878428863463424.0000000000\n",
      "Epoch:100| T Loss:-53457238905053224.0000000000 | V Loss:-349431598535409664.0000000000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class RegNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RegNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(1000, 5000)\n",
    "        self.fc2 = nn.Linear(5000, 10000)\n",
    "        self.fc3 = nn.Linear(10000,3000)\n",
    "        self.fc4 = nn.Linear(3000,100)\n",
    "        self.fc5 = nn.Linear(100,1)\n",
    "        self.thr = nn.ReLU()\n",
    "\n",
    "    def forward(self,x):\n",
    "        x=self.thr(self.fc1(x))\n",
    "        x=self.thr(self.fc2(x))\n",
    "        x=self.thr(self.fc3(x))\n",
    "        x=self.thr(self.fc4(x))\n",
    "        x=self.fc5(x)\n",
    "        return x \n",
    "\n",
    "import hdf5storage\n",
    "mat = hdf5storage.loadmat('Heatwdata.mat') \n",
    "import torch\n",
    "import h5py\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pylab as plt\n",
    "import pickle\n",
    "import scipy.io as sio\n",
    "dtype = torch.double\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "H_right=mat['source']\n",
    "H_wrong=mat['wsource']\n",
    "\n",
    "#Convert into torch arrays\n",
    "Hr=torch.from_numpy(H_right)\n",
    "Hw=torch.from_numpy(H_wrong)\n",
    "\n",
    "nValidation=100\n",
    "nTrain=Hr.shape[1]-nValidation\n",
    "\n",
    "print(nTrain)\n",
    "\n",
    "# initial scrambling \n",
    "perm = torch.randperm(nTrain+nValidation)\n",
    "Hr = Hr[:,perm]\n",
    "Hw = Hw[:,perm]\n",
    "\n",
    "# validation dataset\n",
    "val_r = Hr[:,-nValidation:]\n",
    "val_w = Hw[:,-nValidation:]\n",
    "\n",
    "\n",
    "N=50\n",
    "nEpochs=100\n",
    "\n",
    "net = RegNet()\n",
    "net = net.float()\n",
    "\n",
    "# Loss and optimizer\n",
    "learning_rate = 1e-1\n",
    "criterion = nn.L1Loss() \n",
    "optimizer = torch.optim.Adagrad(net.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "for t in range(nEpochs):\n",
    "    tloss=0\n",
    "    val_loss = 0\n",
    "    perm = torch.randperm(nTrain)\n",
    "    for b_ix in np.arange(0,nTrain,N):\n",
    "        \n",
    "        \n",
    "        ##Pre-processing sparse codes for input \n",
    "        xr=Hr[:,perm[b_ix:b_ix+N]].reshape(N,Hr.shape[0])\n",
    "        xw=Hw[:,perm[b_ix:b_ix+N]].reshape(N,Hw.shape[0])\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        normr = net(xr.float())\n",
    "        normw = net(xw.float())\n",
    "\n",
    "        loss1 = -criterion(normr,normw) \n",
    "        \n",
    "        \n",
    "        # Backward and optimize\n",
    "        \n",
    "        loss1.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            tloss+=loss1.item()\n",
    "            if b_ix % nTrain == 0:\n",
    "                vnormr=net(val_r.reshape(val_r.shape[1],val_r.shape[0]).float())\n",
    "                vnormw=net(val_w.reshape(val_w.shape[1],val_w.shape[0]).float())\n",
    "                vloss = -criterion(vnormr,vnormw).float()\n",
    "                print('Epoch:{:d}| T Loss:{:.10f} | V Loss:{:.10f}'.format(t+1, tloss/nTrain, vloss/nValidation))\n",
    "                tloss=0\n",
    "                vloss=0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
